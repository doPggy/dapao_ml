{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_set():\n",
    "    posting_list =[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    class_vec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return posting_list, class_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据已有数据集，生成一个词汇表\n",
    "def gen_voc_unique_list(data_set):\n",
    "    voc_set = set(())\n",
    "    for post in data_set:\n",
    "        voc_set = voc_set | set(post)\n",
    "    return list(voc_set)\n",
    "\n",
    "# 将一条留言根据词汇表转成一个行向量，0 代表这个词没有出现，1 代表出现\n",
    "def gen_post_vec_by_voc_unique_list(post, voc_set):\n",
    "    post_vec = [0] * len(voc_set)\n",
    "    for word in post:\n",
    "        if word in voc_set:\n",
    "            post_vec[voc_set.index(word)] = 1\n",
    "    return post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "['him', 'mr', 'stupid', 'flea', 'so', 'my', 'please', 'not', 'park', 'stop', 'quit', 'problems', 'has', 'how', 'I', 'take', 'licks', 'ate', 'worthless', 'love', 'help', 'dog', 'posting', 'dalmation', 'garbage', 'buying', 'food', 'cute', 'maybe', 'to', 'is', 'steak']\n"
     ]
    }
   ],
   "source": [
    "data_set, class_vec = gen_data_set()\n",
    "print(data_set)\n",
    "voc_list = gen_voc_unique_list(data_set)\n",
    "print(voc_list)\n",
    "# gen_post_vec_by_voc_unique_list(data_set[0], voc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def post_2_vec(data_set, voc_unique):\n",
    "    train_set = []\n",
    "    for data in data_set:\n",
    "        train_set.append(gen_post_vec_by_voc_unique_list(data, voc_unique))\n",
    "    return train_set\n",
    "\n",
    "post_vec = post_2_vec(data_set, voc_list)\n",
    "print(post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.05263158, 0.        , 0.15789474, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.05263158, 0.05263158, 0.05263158,\n",
       "        0.05263158, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05263158, 0.        , 0.        , 0.10526316, 0.        ,\n",
       "        0.        , 0.10526316, 0.05263158, 0.        , 0.05263158,\n",
       "        0.05263158, 0.05263158, 0.        , 0.05263158, 0.05263158,\n",
       "        0.        , 0.        ]),\n",
       " array([0.08333333, 0.04166667, 0.        , 0.04166667, 0.04166667,\n",
       "        0.125     , 0.04166667, 0.        , 0.        , 0.04166667,\n",
       "        0.        , 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "        0.        , 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "        0.04166667, 0.04166667, 0.        , 0.04166667, 0.        ,\n",
       "        0.        , 0.        , 0.04166667, 0.        , 0.04166667,\n",
       "        0.04166667, 0.04166667]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_pro(post_vec, class_vec):\n",
    "    # 根据贝叶斯公式，计算类别的概率,p(w1,w2,w3, ...| ci)\n",
    "    post_num         = len(post_vec)\n",
    "    word_num_in_post = len(post_vec[0])\n",
    "    p_abusive         = sum(class_vec) / post_num\n",
    "    p_abusive_num     = np.zeros(word_num_in_post)\n",
    "    p_abusive_total   = 0.0\n",
    "    p_not_abusive_num = np.zeros(word_num_in_post)\n",
    "    p_not_abusive_total = 0.0\n",
    "#     print(p_abusive)\n",
    "    \n",
    "    for i in range(post_num):\n",
    "        # 侮辱留言\n",
    "        if class_vec[i] == 1:\n",
    "            # 若标记为侮辱留言，那么要把其中出现在词汇表的词都算作侮辱性词汇来计算概率\n",
    "            p_abusive_num += post_vec[i] # 一个向量中，某些单词出现了，要增加次数\n",
    "            # 计算侮辱性词汇出现总数\n",
    "            p_abusive_total += sum(post_vec[i])\n",
    "        # 非侮辱留言\n",
    "        else:\n",
    "            p_not_abusive_num += post_vec[i]\n",
    "            # 认为出现的都是非侮辱性词汇\n",
    "            p_not_abusive_total += sum(post_vec[i])\n",
    "    return p_abusive_num / p_abusive_total, p_not_abusive_num / p_not_abusive_total, p_abusive\n",
    "    \n",
    "# 可以看到，在给定类别 ci(侮辱与否) 的情况下，p(w0,w1,w2,...|ci)\n",
    "calc_pro(post_vec, class_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目前两个问题\n",
    "\n",
    "1. 存在 p(wi|ci) = 0 情况，最终 p(w|c) 的概率也会变成 0\n",
    "2. 由于概率可能都很小，计算 p(w|c) 的过程中，会造成下溢出变成 0。\n",
    "\n",
    "针对第一种情况，默认每个词都会出现一次，且起码每个分类的词总数为 2.0。\n",
    "\n",
    "针对第二种情况，将 p(w1,..|ci) 用自然对数计算，保留精度的同时，还能进行 ln(a * b) -> ln(a) + ln(b) 计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pro_1(post_vec, class_vec):\n",
    "    # 根据贝叶斯公式，计算类别的概率,p(w1,w2,w3, ...| ci)\n",
    "    post_num         = len(post_vec)\n",
    "    word_num_in_post = len(post_vec[0])\n",
    "    p_abusive         = sum(class_vec) / post_num\n",
    "    p_abusive_num     = np.ones(word_num_in_post) # 默认都会有一次\n",
    "    p_abusive_total   = 2.0 # \n",
    "    p_not_abusive_num = np.ones(word_num_in_post) # 默认都有一次\n",
    "    p_not_abusive_total = 2.0 #\n",
    "    \n",
    "    for i in range(post_num):\n",
    "        # 侮辱留言\n",
    "        if class_vec[i] == 1:\n",
    "            # 若标记为侮辱留言，那么要把其中出现在词汇表的词都算作侮辱性词汇来计算概率\n",
    "            p_abusive_num += post_vec[i] # 一个向量中，某些单词出现了，要增加次数\n",
    "            # 计算侮辱性词汇出现总数\n",
    "            p_abusive_total += sum(post_vec[i])\n",
    "        # 非侮辱留言\n",
    "        else:\n",
    "            p_not_abusive_num += post_vec[i]\n",
    "            # 认为出现的都是非侮辱性词汇\n",
    "            p_not_abusive_total += sum(post_vec[i])\n",
    "    return np.log(p_abusive_num / p_abusive_total), np.log(p_not_abusive_num / p_not_abusive_total), p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_abusive, p_not_abusive, p_class_abusive = calc_pro_1(post_vec, class_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上只是计算了各留言的各词的概率。但是利用贝叶斯分类的目的还没有达成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# test_post = ['love', 'my', 'dalmation']，需要转化成 post_vec 形式\n",
    "# \n",
    "def classify(test_post, p_abusive, p_not_abusive, p_class_abusive):\n",
    "    # sum(p_abusive * test_post) 将 post 中，有出现在词汇表的词都计算进来\n",
    "    p_test_abusive     = sum(p_abusive * test_post) + np.log(p_class_abusive)\n",
    "    p_test_not_abusive = sum(p_not_abusive * test_post) + np.log(1.0 - p_class_abusive)\n",
    "    \n",
    "    if p_test_abusive > p_test_not_abusive:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "# gen_post_vec_by_voc_unique_list(['love', 'my', 'dalmation'], voc_list)\n",
    "test_post = gen_post_vec_by_voc_unique_list(['love', 'my', 'dalmation'], voc_list)\n",
    "\n",
    "print(classify(test_post, p_abusive, p_not_abusive, p_class_abusive))\n",
    "\n",
    "test_post = gen_post_vec_by_voc_unique_list(['stupid', 'garbage'], voc_list)\n",
    "print(classify(test_post, p_abusive, p_not_abusive, p_class_abusive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的 `voc_unique_list` 是词集模型，以一个单词是否出现作为一个特征，但是如果这个词其实是在相同或不同的句子中多次出现，这个模型就不能体现出更多的信息。\n",
    "\n",
    "所以搞了一个词袋模型，其实就是每个特征记录出现的次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一条留言根据词汇表转成一个行向量，记录出现次数\n",
    "def gen_bag_post_vec_by_voc_unique_list(post, voc_set):\n",
    "    post_vec = [0] * len(voc_set)\n",
    "    for word in post:\n",
    "        if word in voc_set:\n",
    "            post_vec[voc_set.index(word)] += 1\n",
    "    return post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1\n",
      "0.0\n",
      "0.0\n",
      "0.2\n",
      "0.1\n",
      "0.0\n",
      "0.1\n",
      "0.0\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "def text_parse(big_string):\n",
    "    # 词条叫一个 token 可能是单词，可能是 url\n",
    "    tokens_list = re.split(r'\\W*', big_string)\n",
    "    # 都要小写，只是这个应用需要, 同时去除空字符\n",
    "    return [ tok.lower() for tok in tokens_list if len(tok) > 2 ]\n",
    "\n",
    "# with open('./email/spam/6.txt', 'r') as f:\n",
    "#     t = text_parse(f.read())\n",
    "def spam():\n",
    "    \n",
    "    full_text_vec = []\n",
    "    class_vec     = []\n",
    "    doc_list      = []\n",
    "    # spam 和 ham 目录都有 25 封邮件\n",
    "    for i in range(1, 26):\n",
    "        # 注意一下编码，真的操\n",
    "        with open('./email/ham/{}.txt'.format(i), 'r', encoding='cp1252') as f:\n",
    "            file_content_vec = text_parse(f.read())\n",
    "            full_text_vec.append(file_content_vec)\n",
    "            doc_list.append(file_content_vec)\n",
    "            class_vec.append(0) # 非垃圾邮件\n",
    "        with open('./email/spam/{}.txt'.format(i), 'r', encoding='cp1252') as f:\n",
    "            file_content_vec = text_parse(f.read())\n",
    "            full_text_vec.append(file_content_vec)\n",
    "            doc_list.append(file_content_vec)\n",
    "            class_vec.append(1) # 垃圾邮件\n",
    "    \n",
    "#     print(class_vec)\n",
    "    # 用 doc list 做词汇表\n",
    "    voc_list = gen_voc_unique_list(doc_list)\n",
    "#     print(len(voc_list))\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    # 将数据集分割成测试集和训练集，10 个测试集 留出法\n",
    "    rand_list = list(range(50))\n",
    "    random.shuffle(rand_list)\n",
    "    for index in rand_list[:10]:\n",
    "        x_test.append(gen_post_vec_by_voc_unique_list(doc_list[index], voc_list))\n",
    "        y_test.append(class_vec[index])\n",
    "        \n",
    "    for index in rand_list[10:]:\n",
    "        x_train.append(gen_post_vec_by_voc_unique_list(doc_list[index], voc_list))\n",
    "        y_train.append(class_vec[index]) \n",
    "        \n",
    "    p_1, p_0, p_spam = calc_pro_1(x_train, y_train) # 训练出模型(就是各个概率计算出来，让贝叶斯分类完整)\n",
    "    \n",
    "    correct = 0.0\n",
    "    wrong   = 0.0\n",
    "    for i in range(10):\n",
    "        result = classify(x_test[i], p_1, p_0, p_spam)\n",
    "#         print(result, y_test[i])\n",
    "        if y_test[i] == result:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    print(wrong / 10.0)\n",
    "    \n",
    "    \n",
    "\n",
    "for i in range(10):\n",
    "    spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "# feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
