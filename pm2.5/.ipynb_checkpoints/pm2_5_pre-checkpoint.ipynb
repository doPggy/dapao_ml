{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>...</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.20</td>\n",
       "      <td>6.60</td>\n",
       "      <td>7.90</td>\n",
       "      <td>...</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>46.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>70.00</td>\n",
       "      <td>66.00</td>\n",
       "      <td>85.00</td>\n",
       "      <td>...</td>\n",
       "      <td>59.00</td>\n",
       "      <td>308.00</td>\n",
       "      <td>327.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>109.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>109.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>36.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>327.00</td>\n",
       "      <td>74.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>83.00</td>\n",
       "      <td>106.00</td>\n",
       "      <td>105.00</td>\n",
       "      <td>...</td>\n",
       "      <td>18.00</td>\n",
       "      <td>311.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>54.00</td>\n",
       "      <td>121.00</td>\n",
       "      <td>97.00</td>\n",
       "      <td>107.00</td>\n",
       "      <td>118.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>1.90</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.80</td>\n",
       "      <td>...</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4320 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2       3      4      5      6      7       8       9  \\\n",
       "0     14.00  14.00  14.00   13.00  12.00  12.00  12.00  12.00   15.00   17.00   \n",
       "1      1.80   1.80   1.80    1.80   1.80   1.80   1.80   1.80    1.80    1.80   \n",
       "2      0.51   0.41   0.39    0.37   0.35   0.30   0.37   0.47    0.78    0.74   \n",
       "3      0.20   0.15   0.13    0.12   0.11   0.06   0.10   0.13    0.26    0.23   \n",
       "4      0.90   0.60   0.50    1.70   1.80   1.50   1.90   2.20    6.60    7.90   \n",
       "...     ...    ...    ...     ...    ...    ...    ...    ...     ...     ...   \n",
       "4315   1.80   1.80   1.80    1.80   1.80   1.70   1.70   1.80    1.80    1.80   \n",
       "4316  46.00  13.00  61.00   44.00  55.00  68.00  66.00  70.00   66.00   85.00   \n",
       "4317  36.00  55.00  72.00  327.00  74.00  52.00  59.00  83.00  106.00  105.00   \n",
       "4318   1.90   2.40   1.90    2.80   2.30   1.90   2.10   3.70    2.80    3.80   \n",
       "4319   0.70   0.80   1.80    1.00   1.90   1.70   2.10   2.00    2.00    1.70   \n",
       "\n",
       "      ...     14      15      16     17      18      19      20      21  \\\n",
       "0     ...  22.00   22.00   21.00  19.00   17.00   16.00   15.00   15.00   \n",
       "1     ...   1.80    1.80    1.80   1.80    1.80    1.80    1.80    1.80   \n",
       "2     ...   0.37    0.37    0.47   0.69    0.56    0.45    0.38    0.35   \n",
       "3     ...   0.10    0.13    0.14   0.23    0.18    0.12    0.10    0.09   \n",
       "4     ...   2.50    2.20    2.50   2.30    2.10    1.90    1.50    1.60   \n",
       "...   ...    ...     ...     ...    ...     ...     ...     ...     ...   \n",
       "4315  ...   1.80    1.80    2.00   2.10    2.00    1.90    1.90    1.90   \n",
       "4316  ...  59.00  308.00  327.00  21.00  100.00  109.00  108.00  114.00   \n",
       "4317  ...  18.00  311.00   52.00  54.00  121.00   97.00  107.00  118.00   \n",
       "4318  ...   2.30    2.60    1.30   1.00    1.50    1.00    1.70    1.50   \n",
       "4319  ...   1.30    1.70    0.70   0.40    1.10    1.40    1.30    1.60   \n",
       "\n",
       "          22      23  \n",
       "0      15.00   15.00  \n",
       "1       1.80    1.80  \n",
       "2       0.36    0.32  \n",
       "3       0.10    0.08  \n",
       "4       1.80    1.50  \n",
       "...      ...     ...  \n",
       "4315    2.00    2.00  \n",
       "4316  108.00  109.00  \n",
       "4317  100.00  105.00  \n",
       "4318    2.00    2.00  \n",
       "4319    1.80    2.00  \n",
       "\n",
       "[4320 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('data/train.csv', encoding = 'Big5')\n",
    "raw_data = raw_data.drop(['日期', '測站', '測項'], axis = 1)\n",
    "# raw_data = raw_data.drop(['日期', '測站'], axis = 1)\n",
    "raw_data = raw_data.replace('NR', -1)\n",
    "raw_data = raw_data.astype(np.float)\n",
    "\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data.at[10, '1']\n",
    "#raw_data.loc[10] 一行的数据 + label\n",
    "ITEM_NUM    = 18 # CO CH4 ... WS_HR 一共 18 个测项\n",
    "PM2_5_INDEX = 9 # pm2.5 的数据都在每一天数据的第 10 行\n",
    "HOUR_NUM    = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok,数据需要进行预处理。在本次模型中，我们就只取前九个时刻的 ```pm2.5``` 数值。\n",
    "\n",
    "\n",
    "所以我只取了所有 ```pm2.5``` 的数据。\n",
    "\n",
    "\n",
    "同时进行特征缩放，可以使用归一化."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9  ...    14  \\\n",
       "0    26.0  39.0  36.0  35.0  31.0  28.0  25.0  20.0  19.0  30.0  ...  36.0   \n",
       "1    21.0  23.0  30.0  30.0  22.0  18.0  13.0  13.0  11.0  22.0  ...  53.0   \n",
       "2    19.0  25.0  27.0  20.0  16.0  14.0  15.0   8.0   4.0   9.0  ...  32.0   \n",
       "3    27.0  27.0  14.0  20.0  22.0  24.0  26.0  33.0  48.0  50.0  ...  62.0   \n",
       "4    80.0  80.0  76.0  81.0  75.0  66.0  70.0  65.0  66.0  57.0  ...  64.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "235   5.0   9.0  17.0  26.0  33.0  32.0  24.0  16.0  16.0  21.0  ...   8.0   \n",
       "236  21.0  26.0  26.0  31.0  29.0  21.0  13.0  13.0  21.0  23.0  ...  12.0   \n",
       "237   0.0   2.0   2.0   3.0   3.0   3.0  11.0  13.0  14.0   8.0  ...  31.0   \n",
       "238  14.0  16.0  13.0  14.0  21.0  19.0  23.0  18.0  17.0  24.0  ...  21.0   \n",
       "239   6.0   1.0   1.0   1.0   0.0   0.0   0.0   2.0   2.0   8.0  ...   1.0   \n",
       "\n",
       "       15    16    17    18    19    20    21    22    23  \n",
       "0    45.0  42.0  49.0  45.0  44.0  41.0  30.0  24.0  13.0  \n",
       "1    43.0  43.0  45.0  46.0  32.0  16.0  19.0  22.0  26.0  \n",
       "2    36.0  34.0  45.0  40.0  41.0  23.0  29.0  23.0  37.0  \n",
       "3    55.0  56.0  67.0  78.0  83.0  90.0  75.0  85.0  82.0  \n",
       "4    73.0  57.0  57.0  53.0  70.0  70.0  60.0  68.0  66.0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "235  16.0  28.0  37.0  39.0  36.0  36.0  31.0  26.0  19.0  \n",
       "236   6.0   4.0  11.0  11.0   5.0   0.0   6.0   6.0   5.0  \n",
       "237  31.0  36.0  31.0  31.0  34.0  39.0  36.0  24.0  23.0  \n",
       "238  21.0  19.0  19.0  25.0  19.0  11.0   0.0   7.0   7.0  \n",
       "239   7.0  24.0  37.0  49.0  37.0  28.0  17.0  24.0  29.0  \n",
       "\n",
       "[240 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pm2_5 = raw_data.loc[[x for x in range(raw_data.shape[0]) if x % ITEM_NUM == PM2_5_INDEX]]\n",
    "all_pm2_5.reset_index(drop = True, inplace = True)\n",
    "# all_pm2_5 = (all_pm2_5 - all_pm2_5.mean()) / all_pm2_5.std() # 默认以 axis = 1 也就是一列方向求 mean/std\n",
    "# print(all_pm2_5[:2])\n",
    "# all_pm2_5[:2].mean().shape # 默认以 axis = 1 也就是一列方向求 mean\n",
    "all_pm2_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在数据预处理，构造一个 ```shape = (..., 9)``` 的 ```train set```。和对应行数的 ```y_train```。\n",
    "\n",
    "\n",
    "```x_train``` 处理的顺序为 ```0 - 8, 1 - 9, 2 - 10 ...``` 以此类推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26. 39. 36. ... 25. 20. 19.]\n",
      " [21. 23. 30. ... 13. 13. 11.]\n",
      " [19. 25. 27. ... 15.  8.  4.]\n",
      " ...\n",
      " [31. 31. 36. ... 39. 36. 24.]\n",
      " [21. 21. 19. ... 11.  0.  7.]\n",
      " [ 1.  7. 24. ... 28. 17. 24.]]\n",
      "[30. 22.  9. ... 23.  7. 29.]\n",
      "(3600, 9)\n",
      "(3600,)\n"
     ]
    }
   ],
   "source": [
    "def train_data_treatment(raw_data):\n",
    "    # 创建一个空的\n",
    "    x_train = np.array([]).reshape(0, HOUR_NUM) \n",
    "    y_train = np.array([]).reshape(0, 1)\n",
    "    for i in range(0, all_pm2_5.shape[1] - HOUR_NUM):\n",
    "        nine_hours_pm2_5 = all_pm2_5[[ str(x) for x in range(i, i + HOUR_NUM) ]].values\n",
    "        next_hour_pm2_5  = all_pm2_5[[ str(i + HOUR_NUM) ]].values\n",
    "        x_train = np.append(x_train, nine_hours_pm2_5, axis = 0)\n",
    "        y_train = np.append(y_train, next_hour_pm2_5, axis = 0)\n",
    "    return x_train, np.squeeze(y_train)\n",
    "    \n",
    "x_train, y_train = train_data_treatment(all_pm2_5)\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就是 train set 与 validation set 的分割\n",
    "\n",
    "我们可以自己决定一个比例，我就定个 ```0.1``` 。(train : vali = 9 : 1) \n",
    "\n",
    "有效集的意义在于，检验模型在 train data 上的表现是否好的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13. 15. 17. ... 12.  9.  6.]\n",
      " [11. 22. 25. ... 39. 37. 28.]\n",
      " [25. 18. 18. ... 15. 15. 18.]\n",
      " ...\n",
      " [21. 17. 14. ... 13. 14. 13.]\n",
      " [38. 37. 31. ... 13. 16. 19.]\n",
      " [ 9.  4.  0. ...  4.  4.  1.]]\n",
      "[ 4. 24. 21. ... 13. 23. 18.]\n"
     ]
    }
   ],
   "source": [
    "def split_train_valid(x_train, y_train, valid_percent = 0.1):\n",
    "    sample_num  = x_train.shape[0]\n",
    "    feature_num = x_train.shape[1]\n",
    "    valid_num   = int(sample_num * 0.1) # 切片得整数\n",
    "    order       = np.arange(sample_num)\n",
    "    np.random.shuffle(order)\n",
    "    x_valid_set, x_train_set = x_train[order][ : valid_num ], x_train[order][ valid_num : ]\n",
    "    y_valid_set, y_train_set = y_train[order][ : valid_num ], y_train[order][ valid_num : ]\n",
    "\n",
    "    return x_train_set, y_train_set, x_valid_set, y_valid_set\n",
    "    \n",
    "x_train_set, y_train_set, x_valid_set, y_valid_set = split_train_valid(x_train, y_train)\n",
    "print(x_train_set)\n",
    "print(y_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, 接下来搞一下各个梯度下降的算法。\n",
    "\n",
    "在这之前，我们还是要熟悉一下前三部\n",
    "\n",
    "1. define function set\n",
    "2. find loss function\n",
    "3. find best function\n",
    "\n",
    "function set 就使用线性回归\n",
    "$$y = w * x + b$$\n",
    "\n",
    "loss function 则是使用\n",
    "$$L(w,b) = 1/(2·num) · sum(y - (w ·x + b))^2$$\n",
    "\n",
    "第三部则就是梯度下降。\n",
    "$$∑(𝑛=1)^10▒2(𝑦 ̂^𝑛−(𝑏+𝑤∙𝑥_𝑐𝑝^𝑛 ))(−𝑥_𝑐𝑝^𝑛 ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03816042 -0.05609154  0.18282653 -0.1770597  -0.02048876  0.44209953\n",
      " -0.55091072  0.03313482  1.10311251]\n",
      "-0.21193553260571144\n"
     ]
    }
   ],
   "source": [
    "# 一般的梯度下降\n",
    "def gd(x_train, y_train, l_rate = 0.00001):\n",
    "#     l_rate        = 0.00001 # 学习率 之前设置为 0.001 结果直接坏掉\n",
    "    sample_num    = x_train.shape[0]\n",
    "    feature_num   = x_train.shape[1]\n",
    "    w             = np.zeros(feature_num)\n",
    "    b             = 0.0\n",
    "    epoch_num     = 30000 # 训练次数\n",
    "    \n",
    "    for i in range(epoch_num):\n",
    "        y_pre  = np.dot(x_train, w) + b # shape = (sample_num, 1)\n",
    "        loss   = y_train - y_pre\n",
    "        # sum 所有的 (y - y_pre)(-x) x为一个 feature, 这个操作一共要 sample_num 次\n",
    "        #  如果不对其求平均，很快数据就会达到 nan\n",
    "        w_grad = np.dot(loss, -x_train) / sample_num # shape = (feature, 1)\n",
    "        b_grad = loss.sum() / sample_num\n",
    "        w      -= l_rate * w_grad\n",
    "        b      -= l_rate * b_grad\n",
    "    return w, b\n",
    "\n",
    "w, b = gd(x_train_set, y_train_set)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顺带记录一下，刚才 l_rate = 0.001，结果大的离谱，说明这个步长是有问题的。\n",
    "\n",
    "所以小小的一个学习率的修改，都能改变训练效果。真的芜湖起飞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了 w 和 b，利用 validation 看一下 error，检验模型在 train data 上的表现如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.70549922005834\n"
     ]
    }
   ],
   "source": [
    "def valid(w, b, x_valid, y_valid):\n",
    "    y          = np.dot(x_valid, w) + b\n",
    "    loss       = y_valid - y\n",
    "    sample_num = x_valid.shape[0]\n",
    "    print(1.0 / (2 * sample_num) * (loss ** 2).sum())\n",
    "    \n",
    "valid(w, b, x_valid_set, y_valid_set)\n",
    "# np.array([1, 2,3 ]) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让梯度变化更加平滑，加入一个正则项。i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04083115 -0.04870834  0.15942984 -0.15441704 -0.01025793  0.39780339\n",
      " -0.51383346  0.04750931  1.07651768]\n",
      "-0.21658056486515506\n",
      "34.357122077285496\n"
     ]
    }
   ],
   "source": [
    "# 正则化\n",
    "def gd_regular(x_train, y_train, lamda2 = 0.1, l_rate = 0.00001):\n",
    "    sample_num    = x_train.shape[0]\n",
    "    feature_num   = x_train.shape[1]\n",
    "    w             = np.zeros(feature_num)\n",
    "    b             = 0.0\n",
    "    epoch_num     = 30000 # 训练次数\n",
    "    \n",
    "    for i in range(epoch_num):\n",
    "        y_pre  = np.dot(x_train, w) + b # shape = (sample_num, 1)\n",
    "        loss   = y_train - y_pre\n",
    "        # sum 所有的 (y - y_pre)(-x) x为一个 feature, 这个操作一共要 sample_num 次\n",
    "        #  如果不对其求平均，很快数据就会达到 nan\n",
    "        w_grad = np.dot(loss, -x_train) / sample_num + 2 * lamda2 * w # shape = (feature, 1)\n",
    "        b_grad = loss.sum() / sample_num\n",
    "        w      -= l_rate * w_grad\n",
    "        b      -= l_rate * b_grad\n",
    "    return w, b\n",
    "\n",
    "def valid_regular(w, b, x_valid, y_valid, lamda2 = 0.1):\n",
    "    y          = np.dot(x_valid, w) + b\n",
    "    loss       = y_valid - y\n",
    "    sample_num = x_valid.shape[0]\n",
    "    print(1.0 / (2 * sample_num) * (loss ** 2).sum() + lamda2 * (w ** 2).sum())\n",
    "\n",
    "lamda2 = 0.5\n",
    "w1, b1 = gd_regular(x_train_set, y_train_set, lamda2)\n",
    "print(w1)\n",
    "print(b1)\n",
    "valid_regular(w1, b1, x_valid_set, y_valid_set, lamda2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了优化模型训练效果，针对学习率和训练速度有对应的方案。\n",
    "\n",
    "1. 学习率要变化\n",
    "2. 随机且多次更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adagrad 是将过去所有的梯度求平方和在开根号，为了关注梯度的极端变化。\n",
    "\n",
    "让学习率变得可变化，而不是一个固定的学习率贯穿全部的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.19600194127107\n"
     ]
    }
   ],
   "source": [
    "def ada(x_train, y_train, l_rate = 0.01, epoch_num = 3000, lamda2 = 0.1):\n",
    "    sample_num           = x_train.shape[0]\n",
    "    feature_num          = x_train.shape[1]\n",
    "    w_grad_square_sum    = np.zeros(feature_num)\n",
    "    b_grad_square_sum    = 0.0\n",
    "    w                    = np.zeros(feature_num)\n",
    "    b                    = 0.0\n",
    "    list_cost            = []\n",
    "    \n",
    "    for i in range(epoch_num):\n",
    "        y_pre                   = np.dot(x_train, w) + b\n",
    "        loss                    = y_train - y_pre\n",
    "        \n",
    "        # 1 / (2 * num) * sum[(y - loss)^2] + lamda * sum[w ^ 2]\n",
    "        w_grad                  = np.dot(loss, -x_train) / sample_num + 2 * lamda2 * w\n",
    "        b_grad                  = loss.sum() / sample_num\n",
    "        w_grad_square_sum       += w_grad ** 2\n",
    "        b_grad_square_sum       += b_grad ** 2\n",
    "#         print(w_grad_square_sum)\n",
    "        w                       -= l_rate / np.sqrt(w_grad_square_sum) * w_grad\n",
    "        b                       -= l_rate / np.sqrt(b_grad_square_sum) * b_grad\n",
    "        \n",
    "        list_cost.append(1.0 / (2 * sample_num) * (loss ** 2).sum() + lamda2 * (w ** 2).sum())\n",
    "    return w, b, list_cost\n",
    "\n",
    "\n",
    "w2, b2, cost = ada(x_train_set, y_train_set, 0.01, 60000, 0)\n",
    "valid_regular(w2, b2, x_valid_set, y_valid_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.plot(*args, scalex=True, scaley=True, data=None, **kwargs)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(cost)), cost, color = 'b', label = 'ada')\n",
    "plt.title('Train Process')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent 使得训练速度更快，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sgd(x_train, y_train, epoch_num = 3000, lamda2 = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
