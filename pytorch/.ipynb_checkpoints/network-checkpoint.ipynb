{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 nn 来建立神经网络。\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#pytorch 的 network 与 layer 都用 nn.Module 拓展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播·\n",
    "\n",
    "every PyTorch nn.Module has a forward() method\n",
    "\n",
    "# 用 pytorch 实现一个神经网络\n",
    "1. Extend the nn.Module base class.\n",
    "2. Define layers as class attributes.\n",
    "3. Implement the forward() method.im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 开始搭建网络，一层一层一层剥开我的心\n",
    "#         self.layer = None\n",
    "        # out_channels 代表了卷积核的数量\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5) # kernel 卷积核\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5) # out_channels 根据卷积核大小\n",
    "        # 全连接层，需要 flatten\n",
    "        self.fc1   = nn.Linear(in_features = 12 * 4 * 4, out_features = 120)\n",
    "        self.fc2   = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out   = nn.Linear(in_features = 60, out_features = 10) # 最终要 10 类别\n",
    "        \n",
    "    def forward(self, t):\n",
    "#         t = self.layer(t) # layer 都有 __call__ 方法，传入张量后有对应的操作\n",
    "        return t\n",
    "    # 重写此方法，用于对象的格式化输出\n",
    "    def __repr__(self):\n",
    "        return \"my \" + super().__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个层都有权重张量和待重写的 `forward` 函数(向前传播函数)\n",
    "\n",
    "同时在 `fashion-mnist` 中， 图片都是灰度图，通道(channel)是 `1`，所以第一卷积层 `in_channels = 1`，如果是三通道，那就是 `3`.\n",
    "\n",
    "由于 `out_channels = 6`, 所以产生的 `feature map` 就有 `6` 个，下一层的输入也就是同样数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Weights - Learnable Parameters In Neural Networks\n",
    "\n",
    "CNN 中的权重，是不断学习变化的参数。神经网络的学习其实也是可学习参数的学习。\n",
    "\n",
    "那么这些参数在 `pytorch` 中存放在哪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Network()\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.1968,  0.1354,  0.1235, -0.1097, -0.1329],\n",
      "          [-0.0807, -0.0398,  0.1980, -0.0680, -0.1397],\n",
      "          [-0.1877, -0.1606,  0.0424, -0.0468,  0.1424],\n",
      "          [ 0.1185,  0.1408, -0.0512, -0.0086, -0.1582],\n",
      "          [-0.1598,  0.0316, -0.1682,  0.1693, -0.1927]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1234, -0.0658, -0.1321,  0.1996,  0.0850],\n",
      "          [ 0.0431, -0.0631,  0.1728, -0.1278, -0.0669],\n",
      "          [-0.1900, -0.1297,  0.1975,  0.0087, -0.0751],\n",
      "          [ 0.1848,  0.0414, -0.0640,  0.1636,  0.1566],\n",
      "          [ 0.0964,  0.0875, -0.0754,  0.0454,  0.1420]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1297, -0.1086, -0.1922, -0.0826, -0.1018],\n",
      "          [ 0.1822, -0.1366,  0.0171,  0.1331, -0.1064],\n",
      "          [ 0.0989, -0.1914,  0.0126,  0.1144,  0.1912],\n",
      "          [ 0.0798,  0.1088, -0.0054,  0.1352,  0.0976],\n",
      "          [-0.0578,  0.0412,  0.1312,  0.0756,  0.0594]]],\n",
      "\n",
      "\n",
      "        [[[-0.1784, -0.0966, -0.0325,  0.0220, -0.0628],\n",
      "          [-0.1778,  0.0267,  0.0457, -0.0408, -0.1348],\n",
      "          [-0.0795, -0.1657,  0.0216, -0.0101, -0.0430],\n",
      "          [ 0.0028,  0.1181, -0.1438, -0.0275,  0.0103],\n",
      "          [-0.0656, -0.1600, -0.0607, -0.1340,  0.1335]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1113,  0.1583,  0.0004, -0.0035, -0.0426],\n",
      "          [-0.1281, -0.1248, -0.0945, -0.1510,  0.1949],\n",
      "          [-0.0899,  0.0951, -0.0124, -0.0853,  0.0482],\n",
      "          [ 0.1505,  0.0137,  0.1367, -0.0501,  0.1116],\n",
      "          [-0.1677,  0.1458, -0.1707, -0.1070,  0.0332]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1868,  0.0014, -0.1327, -0.1215,  0.1536],\n",
      "          [-0.0350,  0.0308,  0.1517,  0.1846,  0.1429],\n",
      "          [ 0.0256,  0.1628, -0.1347,  0.1223,  0.1266],\n",
      "          [-0.1601,  0.0742,  0.0469, -0.1041, -0.0508],\n",
      "          [-0.1361, -0.0717, -0.1358,  0.1978, -0.1926]]]], requires_grad=True)\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight)\n",
    "print(network.conv1.weight.shape)\n",
    "# shape = (6, 1, 5, 5) 6 - out_channels 1 - 单通道 5 5 - 卷积核大小 (Number of filters, Depth, Height, Width)`\n",
    "# Think of this value of 6 here as giving each of the filters some depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Parameter Class\n",
    "\n",
    "用于跟踪神经网络中参数的变化的一个类。The Parameter class extends the tensor class。\n",
    "\n",
    "nn.Module 会搜索成员变量是否是 `Parameter` 的实例，是就追踪他的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our filter has a depth that matches the number of channels.\n",
    "\n",
    "在线性模型中， 有一个权重张量。他的 height 是代表预期的输出的特征数，width 是输入的特征数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 40, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features   = torch.tensor([1, 2, 3, 4])\n",
    "weight_matrix = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3, 4, 5],\n",
    "    [3, 4, 5, 6],\n",
    "])\n",
    "\n",
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何访问神经网络的参数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([12, 6, 5, 5])\n",
      "torch.Size([12])\n",
      "torch.Size([120, 192])\n",
      "torch.Size([120])\n",
      "torch.Size([60, 120])\n",
      "torch.Size([60])\n",
      "torch.Size([10, 60])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in network.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t\t torch.Size([6])\n",
      "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t\t torch.Size([12])\n",
      "fc1.weight \t\t torch.Size([120, 192])\n",
      "fc1.bias \t\t torch.Size([120])\n",
      "fc2.weight \t\t torch.Size([60, 120])\n",
      "fc2.bias \t\t torch.Size([60])\n",
      "out.weight \t\t torch.Size([10, 60])\n",
      "out.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, '\\t\\t', param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型如何工作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性模型中，有一个权重矩阵。在 `nn.Linear` 创建时就创建了，这个可以看源码。大概是以 `tensor(out_feature, in_feature` 创建的。同时注意，权重矩阵是需要将张量传入 `nn.Parameter` 得到的对象。\n",
    "\n",
    "```python\n",
    "self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "```\n",
    "\n",
    "然后我们可以直接调用这个对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.5112, -2.1990,  2.7608], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
    "fc1 = nn.Linear(in_features = 4, out_features = 3, bias = False)\n",
    "fc1(in_features) # 实现了 __call__ 元方法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时我们之后会发现，`forward()` 方法根本不需要我们显示调用，因为在 `nn.Module` 中，已经在 `__call__` 方法中调用了。\n",
    "\n",
    "```python\n",
    "def __call__(self, *input, **kwargs):\n",
    "    for hook in self._forward_pre_hooks.values():\n",
    "        hook(self, input)\n",
    "    if torch._C._get_tracing_state():\n",
    "        result = self._slow_forward(*input, **kwargs)\n",
    "    else:\n",
    "        result = self.forward(*input, **kwargs)\n",
    "    ...\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向传播的实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个网络结构就是：输入层，2 个卷积层，2 个全连接层，一个输出层\n",
    "\n",
    "So a network is just a function\n",
    "\n",
    "在输入全连接层前，输入的张量需要是摊平的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 开始搭建网络，一层一层一层剥开我的心\n",
    "#         self.layer = None\n",
    "        # out_channels 代表了卷积核的数量\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5) # kernel 卷积核\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5) # out_channels 根据卷积核大小\n",
    "        # 全连接层，需要 flatten\n",
    "        self.fc1   = nn.Linear(in_features = 12 * 4 * 4, out_features = 120)\n",
    "        self.fc2   = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out   = nn.Linear(in_features = 60, out_features = 10) # 最终要 10 类别\n",
    "        \n",
    "    def forward(self, t):\n",
    "#         t = self.layer(t) # layer 都有 __call__ 方法，传入张量后有对应的操作\n",
    "        # (1) 输入层\n",
    "        t = t\n",
    "        # (2) 卷积层\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        # (3) 卷积层\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2) # 池化操作，这个是最大池\n",
    "        \n",
    "        # (4) 全连接\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (5) 全连接\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) 输出层\n",
    "        t = self.out(t)\n",
    "        # The softmax function returns a positive probability \n",
    "        # for each of the prediction classes, and the probabilities sum to 1.\n",
    "        #t = F.softmax(t, dim = 1)\n",
    "        \n",
    "        \n",
    "        return t\n",
    "    # 重写此方法，用于对象的格式化输出\n",
    "    def __repr__(self):\n",
    "        return \"my \" + super().__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向传播解释\n",
    "\n",
    "Forward propagation is the process of transforming an input tensor to an output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_set)) \n",
    "image, label = sample\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Network()\n",
    "pred = network(image.unsqueeze(0))\n",
    "pred.shape\n",
    "pred.argmax(dim = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
