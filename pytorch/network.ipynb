{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 nn 来建立神经网络。\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#pytorch 的 network 与 layer 都用 nn.Module 拓展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播·\n",
    "\n",
    "every PyTorch nn.Module has a forward() method\n",
    "\n",
    "# 用 pytorch 实现一个神经网络\n",
    "1. Extend the nn.Module base class.\n",
    "2. Define layers as class attributes.\n",
    "3. Implement the forward() method.im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 开始搭建网络，一层一层一层剥开我的心\n",
    "#         self.layer = None\n",
    "        # out_channels 代表了卷积核的数量\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5) # kernel 卷积核\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5) # out_channels 根据卷积核大小\n",
    "        # 全连接层，需要 flatten\n",
    "        self.fc1   = nn.Linear(in_features = 12 * 4 * 4, out_features = 120)\n",
    "        self.fc2   = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out   = nn.Linear(in_features = 60, out_features = 10) # 最终要 10 类别\n",
    "        \n",
    "    def forward(self, t):\n",
    "#         t = self.layer(t) # layer 都有 __call__ 方法，传入张量后有对应的操作\n",
    "        return t\n",
    "    # 重写此方法，用于对象的格式化输出\n",
    "    def __repr__(self):\n",
    "        return \"my \" + super().__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个层都有权重张量和待重写的 `forward` 函数(向前传播函数)\n",
    "\n",
    "同时在 `fashion-mnist` 中， 图片都是灰度图，通道(channel)是 `1`，所以第一卷积层 `in_channels = 1`，如果是三通道，那就是 `3`.\n",
    "\n",
    "由于 `out_channels = 6`, 所以产生的 `feature map` 就有 `6` 个，下一层的输入也就是同样数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Weights - Learnable Parameters In Neural Networks\n",
    "\n",
    "CNN 中的权重，是不断学习变化的参数。神经网络的学习其实也是可学习参数的学习。\n",
    "\n",
    "那么这些参数在 `pytorch` 中存放在哪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = Network()\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.1911,  0.1132,  0.0521, -0.0519, -0.1630],\n",
      "          [ 0.0017, -0.0731, -0.1816,  0.1704,  0.0231],\n",
      "          [-0.1792,  0.1042, -0.0264,  0.1664,  0.0965],\n",
      "          [-0.1085, -0.1906,  0.1135,  0.0865, -0.1134],\n",
      "          [-0.0709,  0.0220, -0.1816,  0.0121, -0.1762]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0832, -0.1990, -0.0035, -0.1684, -0.0127],\n",
      "          [ 0.0345,  0.0486,  0.0023, -0.0649, -0.0712],\n",
      "          [ 0.1051,  0.1806,  0.0459, -0.0600, -0.1899],\n",
      "          [ 0.0683,  0.1851,  0.1463, -0.1828,  0.0793],\n",
      "          [ 0.0702, -0.1815, -0.0468,  0.1229, -0.1092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1425,  0.1144, -0.1758, -0.0223, -0.0859],\n",
      "          [-0.1328,  0.0022,  0.1742,  0.1733,  0.1452],\n",
      "          [ 0.0505, -0.1239,  0.1721, -0.1935,  0.0584],\n",
      "          [ 0.1759,  0.0215,  0.0017, -0.1775, -0.0105],\n",
      "          [-0.1166,  0.1672, -0.0806, -0.0410, -0.1445]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1080,  0.0560,  0.1909,  0.1307,  0.1864],\n",
      "          [ 0.0658, -0.0558,  0.1964,  0.0633, -0.1071],\n",
      "          [-0.1502,  0.0269, -0.0050, -0.0011,  0.0216],\n",
      "          [-0.0483, -0.0784, -0.1100, -0.0867, -0.1739],\n",
      "          [-0.0624,  0.0801, -0.0400, -0.0889,  0.0055]]],\n",
      "\n",
      "\n",
      "        [[[-0.1594,  0.0453,  0.0881,  0.0849, -0.0861],\n",
      "          [-0.1989,  0.1963,  0.0917, -0.1728,  0.1151],\n",
      "          [-0.1874, -0.1185,  0.0926, -0.1875, -0.0586],\n",
      "          [ 0.1206, -0.1027,  0.0623,  0.0619, -0.0779],\n",
      "          [ 0.1222,  0.1425,  0.0204, -0.0719, -0.0675]]],\n",
      "\n",
      "\n",
      "        [[[-0.0816, -0.1160,  0.0772, -0.0900,  0.1060],\n",
      "          [-0.0769, -0.1022, -0.1016, -0.0644,  0.0916],\n",
      "          [ 0.1002,  0.0903, -0.0378, -0.1175,  0.0897],\n",
      "          [-0.1897,  0.0113,  0.0197,  0.1317, -0.1610],\n",
      "          [-0.0350, -0.1989,  0.1892, -0.1499,  0.0624]]]], requires_grad=True)\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight)\n",
    "print(network.conv1.weight.shape)\n",
    "# shape = (6, 1, 5, 5) 6 - out_channels 1 - 单通道 5 5 - 卷积核大小 (Number of filters, Depth, Height, Width)`\n",
    "# Think of this value of 6 here as giving each of the filters some depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Parameter Class\n",
    "\n",
    "用于跟踪神经网络中参数的变化的一个类。The Parameter class extends the tensor class。\n",
    "\n",
    "nn.Module 会搜索成员变量是否是 `Parameter` 的实例，是就追踪他的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our filter has a depth that matches the number of channels.\n",
    "\n",
    "在线性模型中， 有一个权重张量。他的 height 是代表预期的输出的特征数，width 是输入的特征数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 40, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features   = torch.tensor([1, 2, 3, 4])\n",
    "weight_matrix = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3, 4, 5],\n",
    "    [3, 4, 5, 6],\n",
    "])\n",
    "\n",
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何访问神经网络的参数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([12, 6, 5, 5])\n",
      "torch.Size([12])\n",
      "torch.Size([120, 192])\n",
      "torch.Size([120])\n",
      "torch.Size([60, 120])\n",
      "torch.Size([60])\n",
      "torch.Size([10, 60])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in network.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t\t torch.Size([6])\n",
      "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t\t torch.Size([12])\n",
      "fc1.weight \t\t torch.Size([120, 192])\n",
      "fc1.bias \t\t torch.Size([120])\n",
      "fc2.weight \t\t torch.Size([60, 120])\n",
      "fc2.bias \t\t torch.Size([60])\n",
      "out.weight \t\t torch.Size([10, 60])\n",
      "out.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, '\\t\\t', param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型如何工作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性模型中，有一个权重矩阵。在 `nn.Linear` 创建时就创建了，这个可以看源码。大概是以 `tensor(out_feature, in_feature` 创建的。同时注意，权重矩阵是需要将张量传入 `nn.Parameter` 得到的对象。\n",
    "\n",
    "```python\n",
    "self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "```\n",
    "\n",
    "然后我们可以直接调用这个对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.8247, -0.3079,  2.0328], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
    "fc1 = nn.Linear(in_features = 4, out_features = 3, bias = False)\n",
    "fc1(in_features) # 实现了 __call__ 元方法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时我们之后会发现，`forward()` 方法根本不需要我们显示调用，因为在 `nn.Module` 中，已经在 `__call__` 方法中调用了。\n",
    "\n",
    "```python\n",
    "def __call__(self, *input, **kwargs):\n",
    "    for hook in self._forward_pre_hooks.values():\n",
    "        hook(self, input)\n",
    "    if torch._C._get_tracing_state():\n",
    "        result = self._slow_forward(*input, **kwargs)\n",
    "    else:\n",
    "        result = self.forward(*input, **kwargs)\n",
    "    ...\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向传播的实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个网络结构就是：输入层，2 个卷积层，2 个全连接层，一个输出层\n",
    "\n",
    "So a network is just a function\n",
    "\n",
    "在输入全连接层前，输入的张量需要是摊平的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 开始搭建网络，一层一层一层剥开我的心\n",
    "#         self.layer = None\n",
    "        # out_channels 代表了卷积核的数量\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5) # kernel 卷积核\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5) # out_channels 根据卷积核大小\n",
    "        # 全连接层，需要 flatten\n",
    "        self.fc1   = nn.Linear(in_features = 12 * 4 * 4, out_features = 120)\n",
    "        self.fc2   = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out   = nn.Linear(in_features = 60, out_features = 10) # 最终要 10 类别\n",
    "        \n",
    "    def forward(self, t):\n",
    "#         t = self.layer(t) # layer 都有 __call__ 方法，传入张量后有对应的操作\n",
    "        # (1) 输入层\n",
    "        t = t\n",
    "        # (2) 卷积层\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2)\n",
    "        # (3) 卷积层\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size = 2, stride = 2) # 池化操作，这个是最大池\n",
    "        \n",
    "        # (4) 全连接\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (5) 全连接\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) 输出层\n",
    "        t = self.out(t)\n",
    "        # The softmax function returns a positive probability \n",
    "        # for each of the prediction classes, and the probabilities sum to 1.\n",
    "        #t = F.softmax(t, dim = 1)\n",
    "        \n",
    "        \n",
    "        return t\n",
    "    # 重写此方法，用于对象的格式化输出\n",
    "    def __repr__(self):\n",
    "        return \"my \" + super().__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv1.weight.shape 是 `[6, 1, 5, 5]` 代表了 `[卷积核数量，输入通道，卷积核高，宽]`\n",
    "\n",
    "虽然卷积核有 6 个，但是在代码中用一个张量就可以了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向传播解释\n",
    "\n",
    "Forward propagation is the process of transforming an input tensor to an output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_set)) \n",
    "image, label = sample\n",
    "print(image.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([1])\n",
      "tensor([[0.1036, 0.1122, 0.0897, 0.0899, 0.1097, 0.0959, 0.0966, 0.0911, 0.0993,\n",
      "         0.1120]])\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) \n",
    "network = Network()\n",
    "pred = network(image.unsqueeze(0)) #加入 batch 这个维度\n",
    "print(pred.shape)\n",
    "print(pred.argmax(dim = 1))\n",
    "print(F.softmax(pred, dim =1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x24f960ef4e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size = 10,\n",
    ")\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "images, labels = batch\n",
    "print(images.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0302,  0.1091, -0.1142, -0.1120,  0.0870, -0.0477, -0.0403, -0.0987,\n",
       "         -0.0128,  0.1079],\n",
       "        [ 0.0232,  0.1072, -0.1060, -0.1141,  0.0910, -0.0543, -0.0390, -0.1154,\n",
       "         -0.0088,  0.1051],\n",
       "        [ 0.0216,  0.1122, -0.1137, -0.1137,  0.0830, -0.0516, -0.0430, -0.1031,\n",
       "         -0.0095,  0.1100],\n",
       "        [ 0.0236,  0.1094, -0.1166, -0.1153,  0.0853, -0.0498, -0.0439, -0.1025,\n",
       "         -0.0108,  0.1094],\n",
       "        [ 0.0278,  0.1089, -0.1245, -0.1145,  0.0883, -0.0501, -0.0401, -0.0967,\n",
       "         -0.0103,  0.1072],\n",
       "        [ 0.0283,  0.1058, -0.1097, -0.1135,  0.0877, -0.0568, -0.0423, -0.1094,\n",
       "         -0.0077,  0.1053],\n",
       "        [ 0.0265,  0.1070, -0.1085, -0.1134,  0.0869, -0.0551, -0.0382, -0.1094,\n",
       "         -0.0112,  0.1123],\n",
       "        [ 0.0304,  0.1024, -0.1141, -0.1191,  0.0925, -0.0587, -0.0411, -0.1050,\n",
       "         -0.0093,  0.1047],\n",
       "        [ 0.0278,  0.1025, -0.1116, -0.1086,  0.0926, -0.0580, -0.0355, -0.1027,\n",
       "         -0.0052,  0.0987],\n",
       "        [ 0.0297,  0.1034, -0.1158, -0.1067,  0.0901, -0.0564, -0.0402, -0.1053,\n",
       "         -0.0060,  0.1052]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = network(images)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[batch_size, 各个类别的可能性]`\n",
    "\n",
    "利用 `argmax` 对第二个轴查找最大可能的 `index`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 9, 9, 1, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor(0)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(preds.argmax(dim = 1).eq(labels))\n",
    "print(preds.argmax(dim = 1).eq(labels).sum()) # 计算 True 的数量\n",
    "print(preds.argmax(dim = 1).eq(labels).sum().item()) # 计算 True 的数量,并取出标量值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一层一层讲解\n",
    "\n",
    "## 第一层卷积\n",
    "操作前\n",
    "```python\n",
    "torch.Size([1, 1, 28, 28])\n",
    "```\n",
    "\n",
    "经过卷积操作后\n",
    "```python\n",
    "torch.Size([1, 6, 24, 24])\n",
    "```\n",
    "\n",
    "`batch_size` 依旧是 `1`。The batch_size is fixed as we move through the forward pass.\n",
    "\n",
    "而这里只是刚弄完卷积操作。然后看一下各个轴长。\n",
    "\n",
    "`1` 是 batch_size.\n",
    "\n",
    "`6` 是  out_channels\n",
    "\n",
    "`24` 因为 stride = 2, height/width = 2, 经过卷积操作，`28 - 5 -1 + 2 = width - kernel_size - stride + 2` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实这个层里头的权重张量，就是卷积核们。(还记得每个层都有权重张量吗。\n",
    "\n",
    "```\n",
    "The filters are the weight tensors.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Output Size Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
